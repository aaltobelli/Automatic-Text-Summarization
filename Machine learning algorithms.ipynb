{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import csv\r\n",
    "from sklearn import svm, metrics, naive_bayes\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from rouge_score import rouge_scorer\r\n",
    "import requests\r\n",
    "import statistics as st\r\n",
    "\r\n",
    "#A function from the other file that is also needed here\r\n",
    "def get_doc(collection, docId):\r\n",
    "    r = requests.get('http://localhost:8984/solr/'+collection+'/select?q=docId:'+str(docId)+'&wt=json').json()\r\n",
    "    return r['response']['docs'][0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Matrix Import\r\n",
    "The feature matrix needs to be properly pre-processed into a classifier-compatible structure. Moreover, as stated in the paper, the dataset is split in test set (the first 100 documents) and training set (the other documents). These operations are all implemented in the following code:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with open('features.csv') as csv_file:\r\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\r\n",
    "    sentences, data, classes = ([] for i in range(3))\r\n",
    "    sentencesTest, dataTest, classesTest = ([] for i in range(3))\r\n",
    "    line_count = 0\r\n",
    "    for row in csv_reader:\r\n",
    "        if line_count == 0:\r\n",
    "            features = row\r\n",
    "            line_count += 1\r\n",
    "        elif int(row[1]) < 100:\r\n",
    "            sentencesTest.append(row[0])\r\n",
    "            dataTest.append(row[1:12])\r\n",
    "            classesTest.append(row[12])\r\n",
    "            line_count += 1\r\n",
    "        else:\r\n",
    "            sentences.append(row[0])\r\n",
    "            data.append(row[1:12])\r\n",
    "            classes.append(row[12])\r\n",
    "            line_count += 1\r\n",
    "\r\n",
    "#Transforms the boolean value of the class in a integer\r\n",
    "target = []\r\n",
    "targetTest = []\r\n",
    "for elem in classes:\r\n",
    "    if elem == 'True':\r\n",
    "        target.append(1)\r\n",
    "    else: target.append(0)\r\n",
    "for elem in classesTest:\r\n",
    "    if elem == 'True':\r\n",
    "        targetTest.append(1)\r\n",
    "    else: targetTest.append(0)\r\n",
    "\r\n",
    "for i in range(0, len(data)):\r\n",
    "    if data[i][6] == 'True':\r\n",
    "        data[i][6] = 1\r\n",
    "    else: data[i][6] = 0\r\n",
    "    for j in range(0, len(data[i])):\r\n",
    "        data[i][j] = float(data[i][j])\r\n",
    "        \r\n",
    "for i in range(0, len(dataTest)):\r\n",
    "    if dataTest[i][6] == 'True':\r\n",
    "        dataTest[i][6] = 1\r\n",
    "    else: dataTest[i][6] = 0\r\n",
    "    for j in range(0, len(dataTest[i])):\r\n",
    "        dataTest[i][j] = float(dataTest[i][j])\r\n",
    "\r\n",
    "X_train = data\r\n",
    "y_train = target\r\n",
    "X_test = dataTest\r\n",
    "y_test = targetTest"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "modelSVM = svm.NuSVC() \r\n",
    "modelSVM.fit(X_train, y_train)\r\n",
    "y_predSVM = modelSVM.predict(X_test)\r\n",
    "print(\"SVM Accuracy:\",metrics.accuracy_score(y_test, y_predSVM))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SVM Accuracy: 0.6121134020618557\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive Bayes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "modelNB = naive_bayes.GaussianNB()\r\n",
    "modelNB.fit(X_train, y_train)\r\n",
    "y_predNB = modelNB.predict(X_test)\r\n",
    "print(\"NB Accuracy:\",metrics.accuracy_score(y_test, y_predNB))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NB Accuracy: 0.7371134020618557\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decision Tree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "modelDT = DecisionTreeClassifier()\r\n",
    "modelDT.fit(X_train, y_train)\r\n",
    "y_predDT = modelDT.predict(X_test)\r\n",
    "print(\"DT Accuracy:\",metrics.accuracy_score(y_test, y_predDT))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DT Accuracy: 0.6378865979381443\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summary Generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "results = []\r\n",
    "for i in range(0, len(dataTest)):\r\n",
    "    results.append({'docId':int(dataTest[i][0]), 'sentence':sentencesTest[i], \r\n",
    "                    'SVM':y_predSVM[i], 'NB':y_predNB[i], 'DT':y_predDT[i]})\r\n",
    "    \r\n",
    "summariesSVM = []\r\n",
    "for i in range(0, 100):\r\n",
    "    doc = [d for d in results if d['docId'] == i]\r\n",
    "    summariesSVM.append({'docId':i, 'hypotesis':\" \".join([d['sentence'] for d in doc if d['SVM'] == 1]),\r\n",
    "                        'reference': get_doc('myDocs', i)['summary']})\r\n",
    "\r\n",
    "summariesNB = []\r\n",
    "for i in range(0, 100):\r\n",
    "    doc = [d for d in results if d['docId'] == i]\r\n",
    "    summariesNB.append({'docId':i, 'hypotesis':\" \".join([d['sentence'] for d in doc if d['NB'] == 1]),\r\n",
    "                        'reference': get_doc('myDocs', i)['summary']})\r\n",
    "\r\n",
    "summariesDT = []\r\n",
    "for i in range(0, 100):\r\n",
    "    doc = [d for d in results if d['docId'] == i]\r\n",
    "    summariesDT.append({'docId':i, 'hypotesis':\" \".join([d['sentence'] for d in doc if d['DT'] == 1]), \r\n",
    "                         'reference': get_doc('myDocs', i)['summary']})\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def mean_score(summaries, rouge, scope):\r\n",
    "    scorer = rouge_scorer.RougeScorer([rouge])\r\n",
    "    scores = []\r\n",
    "    for i in range(0, 100):\r\n",
    "        hyp = summaries[i]['hypotesis']\r\n",
    "        ref = summaries[i]['reference']\r\n",
    "        if scope == \"precision\":\r\n",
    "            scores.append(scorer.score(hyp, ref)[rouge].precision)\r\n",
    "        elif scope == \"recall\":\r\n",
    "            scores.append(scorer.score(hyp, ref)[rouge].recall)\r\n",
    "        elif scope == \"fmeasure\":\r\n",
    "            scores.append(scorer.score(hyp, ref)[rouge].fmeasure)\r\n",
    "    return round(st.mean(scores), 4)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}